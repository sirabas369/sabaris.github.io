<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="x-ua-compatible" content="ie=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Sabarish's Homepage</title>
    <meta
      name="keywords"
      content="Sabariswaran Mani, sabaris, sabarish, waran, IIT Kharagpur, KGP, IIT KGP, AI, Robotics, AGV, Quant Club" />
    <meta
      name="description"
      content="Sabariswaran Mani's homepage. Sabariswaran Mani is an Undergraduate Student at IIT Kharagpur" />
    <meta
      property="og:image"
      content="./images/about-img.jpg" />
    <meta name="author" content="Sabariswaran Mani" />
    <meta property="og:site_name" content="Sabarish's Homepage" />
    <meta property="og:title" content="Sabarish's Homepage" />
    <script type="application/ld+json">
      {
        "@context": "https://schema.org",
        "@type": "Person",
        "name": "Sabariswaran Mani",
        "additionalName": "sabarish",
        "url": "https://sirabas369.github.io/",
        "image": "https://github.com/sirabas369/sirabas369.github.io/images/about-img.jpg",
        "jobTitle": "AI Student Researcher",
        "affiliation": {
          "@type": "Organization",
          "name": "Indian Institute of Technology Kharagpur"
        },
        "colleague": [
          {
            "@type": "Person",
            "name": "Venkatesh Babu R"
          },
          {
            "@type": "Person",
            "name": "Debashish Chakravarty"
          }
        ],
        "worksFor": {
          "@type": "Organization",
          "name": "Indian Institute of Technology Kharagpur"
        },
        "sameAs": [
          "https://www.linkedin.com/in/sabariswaran-mani-48b8b522a/",
          "https://scholar.google.com/citations?user=Klwb85AAAAAJ",
          "https://github.com/sirabas369",
        ],
        "memberOf": [
          {
            "@type": "Organization",
            "name": "Indian Institute of Technology Kharagpur"
          },
          {
            "@type": "Organization",
            "name": "Autonomous Ground Vehicles Research Group"
          },
          {
            "@type": "Organization",
            "name": "Quant Club"
          }
        ]
      }
    </script>

    <!-- favicon -->
    <link href="./images/favicon.ico" rel="icon" type="image/x-icon" />

    <!--Preload font-->
    <link
      href="https://fonts.googleapis.com/css?family=Raleway:200,300,400,500,600,700,800,900%7cOpen+Sans:400,600,700,800"
      rel="preload"
      as="style" />

    <!--Bootstrap css-->
    <link rel="stylesheet" href="css/bootstrap.css" />

    <!--Magnific Popup css-->
    <!-- <link rel="stylesheet" href="css/magnific-popup.css" /> -->

    <!-- Google Fonts -->
    <link
      href="https://fonts.googleapis.com/css?family=Raleway:200,300,400,500,600,700,800,900%7cOpen+Sans:400,600,700,800"
      rel="stylesheet" />

    <!--Site Main Style css-->
    <link rel="stylesheet" href="css/style.css" />
  </head>


  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-D4FCT86YQ5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-D4FCT86YQ5');
  </script>

  <body>
    <!--Preloader-->
    <!-- <div class="preloader">
      <div class="loader"></div>
    </div> -->
    <!--Preloader-->

    <!--Navbar Start-->
    <nav id="navbar" class="navbar navbar-expand-lg navbar-dark">
      <div class="container">
        <!-- LOGO -->
        <a class="navbar-brand logo" href="index.html">
          <!-- Home -->
          <!-- <img src="images/Boyuan_Stamp.png" style="height: 80px" /> -->
        </a>

        <button
          class="navbar-toggler collapsed"
          type="button"
          data-toggle="collapse"
          data-target="#navbarCollapse"
          aria-controls="navbarCollapse"
          aria-expanded="false"
          aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>

        <div class="navbar-collapse collapse" id="navbarCollapse">
          <ul class="navbar-nav ml-auto">
            <!--Nav Links-->
            <li class="nav-item">
              <a href="#" class="nav-link active" data-scroll-nav="0">Home</a>
            </li>
            <li class="nav-item">
              <a href="#" class="nav-link" data-scroll-nav="1">About</a>
            </li>
            <li class="nav-item">
              <a href="files/Boyuan_Chen_CV.pdf" class="nav-link">Resume</a>
            </li>
            <li class="nav-item">
              <a href="#" class="nav-link" data-scroll-nav="2">Research</a>
            </li>
            <li class="nav-item">
              <a href="#" class="nav-link" data-scroll-nav="3">Projects</a>
            </li>
            <li class="nav-item">
              <a href="#" class="nav-link" data-scroll-nav="4">Blog</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>
    <!--Navbar End-->

    <!--Home Section Start-->
    <section
      id="home"
      class="banner"
      style="background-image: url('images/background/home-banner-bg.webp')"
      data-stellar-background-ratio=".7"
      data-scroll-index="0">
      <div class="container">
        <!--Banner Content-->
        <div class="banner-caption">
          <h1>Hi! I'm Boyuan Chen.</h1>
          <p class="cd-headline clip mt-30">
            <span>AI Researcher and Roboticist at MIT EECS.</span><br />
            <span>Specialized in</span>
            <span class="cd-words-wrapper">
              <b class="is-visible">machine learning.</b>
              <b>generative models.</b>
              <b>reinforcement learning.</b>
              <b>robotics.</b>
            </span>
          </p>
        </div>
        <div class="arrow">
          <a class="fa fa-chevron-down fa-2x" href="#" data-scroll-nav="1"></a>
        </div>
      </div>
    </section>
    <!--Home Section End-->

    <!--Font Awesome css (the commented out font awesome 4.7 doesn't contain google scholar)-->
    <!-- <link rel="stylesheet" href="css/font-awesome.min.css" /> -->
    <script
      src="https://kit.fontawesome.com/32569faa05.js"
      crossorigin="anonymous"></script>

    <!--About Section Start-->
    <section
      id="about"
      class="about pt-100 pb-100"
      data-scroll-index="1"
      style="display: none">
      <div class="container">
        <div class="row">
          <div class="col-lg-5 col-md-6">
            <!--About Image-->
            <div class="about-img">
              <img
                src="images/about-img.jpg"
                alt="Photo of Boyuan Chen"
                width="550"
                height="600" />
            </div>
          </div>
          <div class="col-lg-7 col-md-6">
            <!--About Content-->
            <div class="about-content">
              <div class="about-heading">
                <h2>About Me</h2>
                <!-- <span>AI/ML Researcher & Robotics Enthusiast</span> -->
              </div>
              <p>
                I'm Boyuan Chen (陈博远), an AI researcher and roboticist at
                MIT. I am currently a third year PhD student working with Prof.
                Vincent Sitzmann and Prof. Russ Tedrake. I am interested in
                foundation models for decision making. I work on building world
                models that allow AI agents to aquire skills via both search
                (like AlphaGo) and policy (like GPT). I am also interested in
                deploying these models on real-world robots, aka embodied
                intelligence. Outside research, building robots is my long-time
                hobby. Scroll down to see some of my robots!
              </p>
              <p>
                Previosuly, I interned at Google Deepmind and Google X. I
                obtained my bachelor's degree in computer science and math at UC
                Berkeley, where I spent a signficant amount of time doing
                research at Berkeley Artificial Intelligence Research (BAIR) on
                deep reinforcement learning and unsupervised learning. I also
                spent a year studying philosophy during my undergrad. I am a big
                fan of chess, robots and boba.
              </p>
              <!--About Social Icons-->
              <div style="display: flex; justify-content: space-between">
                <div class="about-button">
                  <a class="main-btn" href="files/Boyuan_Chen_CV.pdf"
                    >Resume / CV</a
                  >
                </div>
                <div class="social-icons">
                  <a
                    href="https://scholar.google.com/citations?user=rEL4-fgAAAAJ"
                    ><i class="fa fa-brands fa-google-scholar"> </i>
                  </a>
                  <a href="https://twitter.com/BoyuanChen0"
                    ><i class="fa fa-twitter"></i
                  ></a>
                  <a href="https://github.com/buoyancy99/"
                    ><i class="fa fa-github"></i
                  ></a>
                  <a href="https://www.linkedin.com/in/boyuan99/"
                    ><i class="fa fa-linkedin"></i
                  ></a>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!--About Section End-->

    <!--Research Section Start-->
    <section
      id="research"
      class="research bg-gray pt-100 pb-50"
      data-scroll-index="2"
      style="display: none">
      <div class="container">
        <div class="row">
          <div class="col">
            <div class="heading text-center">
              <h2>My research</h2>
            </div>
          </div>
        </div>

        <!--Paper 8 -->
        <div class="row">
          <div class="col-md-4">
            <div class="paper-thumbnail">
              <a href="https://boyuan.space/diffusion-forcing/">
                <img
                  loading="lazy"
                  src="images/research/df.png"
                  alt="paper thumbnail"
                  width="504"
                  height="300" />
              </a>
            </div>
          </div>
          <div class="col-md-8">
            <div class="paper-info" id="df">
              <a href="https://boyuan.space/diffusion-forcing/">
                <heading
                  >Diffusion Forcing: Next-token Prediction Meets Full-Sequence
                  Diffusion
                </heading>
              </a>
              <br />
              Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ
              Tedrake, Vincent Sitzmann
              <br />
              <strong>NeurIPS 2024</strong> (Conference of Neural Information
              Processing Systems)
              <br />
              <br />
              <a href="https://boyuan.space/diffusion-forcing/">website</a>
              |
              <a href="https://arxiv.org/abs/2407.01392">paper</a>
              |
              <a href="javascript:toggleblock('df_abs')">abstract</a> |
              <a
                shape="rect"
                href="javascript:togglebib('df')"
                class="togglebib"
                >bibtex</a
              >
              <pre xml:space="preserve" style="display: none">
@misc{chen2024diffusionforcingnexttokenprediction,
    title={Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion}, 
    author={Boyuan Chen and Diego Marti Monso and Yilun Du and Max Simchowitz and Russ Tedrake and Vincent Sitzmann},
    year={2024},
    eprint={2407.01392},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2407.01392}, 
}
              </pre>
            </div>
            <p style="text-align: justify">
              <i id="df_abs"
                >This paper presents Diffusion Forcing, a new training paradigm
                where a diffusion model is trained to denoise a set of tokens
                with independent per-token noise levels. We apply Diffusion
                Forcing to sequence generative modeling by training a causal
                next-token prediction model to generate one or several future
                tokens without fully diffusing past ones. Our approach is shown
                to combine the strengths of next-token prediction models, such
                as variable-length generation, with the strengths of
                full-sequence diffusion models, such as the ability to guide
                sampling to desirable trajectories. Our method offers a range of
                additional capabilities, such as (1) rolling-out sequences of
                continuous tokens, such as video, with lengths past the training
                horizon, where baselines diverge and (2) new sampling and
                guiding schemes that uniquely profit from Diffusion Forcing's
                variable-horizon and causal architecture, and which lead to
                marked performance gains in decision-making and planning tasks.
                In addition to its empirical success, our method is proven to
                optimize a variational lower bound on the likelihoods of all
                subsequences of tokens drawn from the true joint distribution.
              </i>
            </p>
          </div>
        </div>

        <!--Paper 7 -->
        <div class="row">
          <div class="col-md-4">
            <div class="paper-thumbnail">
              <a href="https://dittogym.github.io/">
                <img
                  loading="lazy"
                  src="images/research/ditto.jpg"
                  alt="paper thumbnail"
                  width="504"
                  height="300" />
              </a>
            </div>
          </div>
          <div class="col-md-8">
            <div class="paper-info" id="dittogym">
              <a href="https://dittogym.github.io/">
                <heading
                  >DittoGym: Learning to Control Soft Shape-Shifting Robots
                </heading>
              </a>
              <br />
              Suning Huang, Boyuan Chen, Huazhe Xu, Vincent Sitzmann
              <br />
              <strong>ICLR 2024</strong> (International Conference on Learning
              Representations)
              <br />
              <br />
              <a href="https://dittogym.github.io/">website</a>
              |
              <a href="https://arxiv.org/abs/2401.13231">paper</a>
              |
              <a href="javascript:toggleblock('dittogym_abs')">abstract</a> |
              <a
                shape="rect"
                href="javascript:togglebib('dittogym')"
                class="togglebib"
                >bibtex</a
              >
              <pre xml:space="preserve" style="display: none">
@misc{huang2024dittogym,
  title={DittoGym: Learning to Control Soft Shape-Shifting Robots}, 
  author={Suning Huang and Boyuan Chen and Huazhe Xu and Vincent Sitzmann},
  year={2024},
  eprint={2401.13231},
  archivePrefix={arXiv},
  primaryClass={cs.RO}
}
              </pre>
            </div>
            <p style="text-align: justify">
              <i id="dittogym_abs"
                >Robot co-design, where the morphology of a robot is optimized
                jointly with a learned policy to solve a specific task, is an
                emerging area of research. It holds particular promise for soft
                robots, which are amenable to novel manufacturing techniques
                that can realize learned morphologies and actuators. Inspired by
                nature and recent novel robot designs, we propose to go a step
                further and explore the novel reconfigurable robots, defined as
                robots that can change their morphology within their lifetime.
                We formalize control of reconfigurable soft robots as a
                highdimensional reinforcement learning (RL) problem. We unify
                morphology change, locomotion, and environment interaction in
                the same action space, and introduce an appropriate,
                coarse-to-fine curriculum that enables us to discover policies
                that accomplish fine-grained control of the resulting robots. We
                also introduce DittoGym, a comprehensive RL benchmark for
                reconfigurable soft robots that require fine-grained morphology
                changes to accomplish the tasks. Finally, we evaluate our
                proposed coarse-to-fine algorithm on DittoGym and demonstrate
                robots that learn to change their morphology several times
                within a sequence, uniquely enabled by our RL algorithm.
              </i>
            </p>
          </div>
        </div>

        <!--Paper 6 -->
        <div class="row">
          <div class="col-md-4">
            <div class="paper-thumbnail">
              <a href="https://spatial-vlm.github.io/">
                <img
                  loading="lazy"
                  src="images/research/spatial_vlm.jpg"
                  alt="paper thumbnail"
                  width="504"
                  height="300" />
              </a>
            </div>
          </div>
          <div class="col-md-8">
            <div class="paper-info" id="spatialvlm">
              <a href="https://spatial-vlm.github.io/">
                <heading
                  >SpatialVLM: Endowing Vision-Language Models with Spatial
                  Reasoning Capabilities
                </heading>
              </a>
              <br />
              Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess,
              Pete Florence, Dorsa Sadigh, Leonidas Guibas, Fei Xia
              <br />
              <strong>CVPR 2024</strong> (Conference on Computer Vision and
              Pattern Recognition)
              <br />
              <br />
              <a href="https://spatial-vlm.github.io/">website</a>
              |
              <a href="https://arxiv.org/abs/2401.12168">paper</a>
              |
              <a href="javascript:toggleblock('spatialvlm_abs')">abstract</a> |
              <a
                shape="rect"
                href="javascript:togglebib('spatialvlm')"
                class="togglebib"
                >bibtex</a
              >
              <pre xml:space="preserve" style="display: none">
@InProceedings{Chen_2024_CVPR,
    author    = {Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brain and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
    title     = {SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {14455-14465}
}
              </pre>
            </div>
            <p style="text-align: justify">
              <i id="spatialvlm_abs"
                >Understanding and reasoning about spatial relationships is a
                fundamental capability for Visual Question Answering (VQA) and
                robotics. While Vision Language Models (VLM) have demonstrated
                remarkable performance in certain VQA benchmarks, they still
                lack capabilities in 3D spatial reasoning, such as recognizing
                quantitative relationships of physical objects like distances or
                size differences. We hypothesize that VLMs' limited spatial
                reasoning capability is due to the lack of 3D spatial knowledge
                in training data and aim to solve this problem by training VLMs
                with Internet-scale spatial reasoning data. To this end, we
                present a system to facilitate this approach. We first develop
                an automatic 3D spatial VQA data generation framework that
                scales up to 2 billion VQA examples on 10 million real-world
                images. We then investigate various factors in the training
                recipe, including data quality, training pipeline, and VLM
                architecture. Our work features the first internet-scale 3D
                spatial reasoning dataset in metric space. By training a VLM on
                such data, we significantly enhance its ability on both
                qualitative and quantitative spatial VQA. Finally, we
                demonstrate that this VLM unlocks novel downstream applications
                in chain-of-thought spatial reasoning and robotics due to its
                quantitative estimation capability.
              </i>
            </p>
          </div>
        </div>

        <!--Paper 5 -->
        <div class="row">
          <div class="col-md-4">
            <div class="paper-thumbnail">
              <a href="https://buoyancy99.github.io/ramp-rl/">
                <img
                  loading="lazy"
                  src="images/research/ramp.jpg"
                  alt="paper thumbnail"
                  width="504"
                  height="300" />
              </a>
            </div>
          </div>
          <div class="col-md-8">
            <div class="paper-info" id="ramp">
              <a href="https://buoyancy99.github.io/ramp-rl/">
                <heading
                  >Self-Supervised Reinforcement Learning that Transfers using
                  Random Features
                </heading>
              </a>
              <br />
              Boyuan Chen, Chuning Zhu, Pulkit Agrawal, Kaiqing Zhang, Abhishek
              Gupta
              <br />
              <strong>NeurIPS 2023</strong> (Conference of Neural Information
              Processing Systems)
              <br />
              <br />
              <a href="https://buoyancy99.github.io/ramp-rl/">website</a>
              |
              <a href="https://arxiv.org/abs/2305.17250">paper</a>
              | <a href="javascript:toggleblock('ramp_abs')">abstract</a> |
              <a
                shape="rect"
                href="javascript:togglebib('ramp')"
                class="togglebib"
                >bibtex</a
              >
              <pre xml:space="preserve" style="display: none">
@misc{chen2023selfsupervised,
  title={Self-Supervised Reinforcement Learning that Transfers using Random Features}, 
  author={Boyuan Chen and Chuning Zhu and Pulkit Agrawal and Kaiqing Zhang and Abhishek Gupta},
  year={2023},
  eprint={2305.17250},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
              </pre>
            </div>
            <p style="text-align: justify">
              <i id="ramp_abs"
                >Reinforcement learning (RL) algorithms have the potential not
                only for synthesizing complex control behaviors, but also for
                transfer across tasks. Model-free RL excels in solving problems
                with high-dimensional observations or long horizons, but the
                learned policies do not transfer across different reward
                functions. Model-based RL, on the other hand, naturally enables
                transfer across different reward functions, but struggles in
                complex environments due to compounding error. In this work, we
                propose a new method for transferring behaviors across tasks
                with different rewards, combining the performance of model-free
                RL with the transferability of model-based RL. In particular, we
                show how model-free RL using a number of random features as the
                reward allows for implicit modeling of long-horizon environment
                dynamics. Model-predictive control using these implicit models
                enables fast adaptation to problems with new reward functions
                while avoiding the compounding error from model rollouts. Our
                method can be trained on offline datasets without reward labels,
                and quickly deployed on new tasks, making it more widely
                applicable than typical RL methods. We validate that our
                proposed method enables transfer across tasks on a variety of
                manipulation and locomotion domains.
              </i>
            </p>
          </div>
        </div>

        <!--Paper 4 -->
        <div class="row">
          <div class="col-md-4">
            <div class="paper-thumbnail">
              <a href="https://sites.google.com/view/eil-website">
                <img
                  loading="lazy"
                  src="images/research/eil.png"
                  alt="paper thumbnail"
                  width="504"
                  height="300" />
              </a>
            </div>
          </div>
          <div class="col-md-8">
            <div class="paper-info" id="eil">
              <a href="https://sites.google.com/view/eil-website">
                <heading>Extraneousness-Aware Imitation Learning </heading>
              </a>
              <br />
              Ray Chen Zheng, Kaizhe Hu, Zhecheng Yuan, Boyuan Chen, Huazhe Xu
              <br />
              <strong>ICRA 2023</strong> (International Conference on Robotics
              and Automation)
              <br />
              <br />
              <a href="https://sites.google.com/view/eil-website">website</a>
              | <a href="https://arxiv.org/pdf/2210.01379v1.pdf">paper</a> |
              <a href="javascript:toggleblock('eil_abs')">abstract</a> |
              <a
                shape="rect"
                href="javascript:togglebib('eil')"
                class="togglebib"
                >bibtex</a
              >
              |
              <a
                href="https://drive.google.com/file/d/18JSM34ChoVIZtMsJStrc3Oal6fsS5muj/view"
                >talk video</a
              >
              <pre xml:space="preserve" style="display: none">
@misc{https://doi.org/10.48550/arxiv.2210.01379,
  doi = {10.48550/ARXIV.2210.01379},
  url = {https://arxiv.org/abs/2210.01379},
  author = {Zheng, Ray Chen and Hu, Kaizhe and Yuan, Zhecheng and Chen, Boyuan and Xu, Huazhe},
  keywords = {Robotics (cs.RO), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Extraneousness-Aware Imitation Learning},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
              </pre>
            </div>
            <p style="text-align: justify">
              <i id="eil_abs"
                >Visual imitation learning provides an effective framework to
                learn skills from demonstrations. However, the quality of the
                provided demonstrations usually significantly affects the
                ability of an agent to acquire desired skills. Therefore, the
                standard visual imitation learning assumes near-optimal
                demonstrations, which are expensive or sometimes prohibitive to
                collect. Previous works propose to learn from noisy
                demonstrations; however, the noise is usually assumed to follow
                a context-independent distribution such as a uniform or gaussian
                distribution. In this paper, we consider another crucial yet
                underexplored setting — imitation learning with task-irrelevant
                yet locally consistent segments in the demonstrations (e.g.,
                wiping sweat while cutting potatoes in a cooking tutorial). We
                argue that such noise is common in real world data and term them
                as “extraneous” segments. To tackle this problem, we introduce
                Extraneousness-Aware Imitation Learning (EIL), a self-supervised
                approach that learns visuomotor policies from third-person
                demonstrations with extraneous subsequences. EIL learns
                action-conditioned observation embeddings in a self-supervised
                manner and retrieves task-relevant observations across visual
                demonstrations while excluding the extraneous ones. Experimental
                results show that EIL outperforms strong baselines and achieves
                comparable policies to those trained with perfect demonstration
                on both simulated and real-world robot control tasks.</i
              >
            </p>
          </div>
        </div>

        <!--Paper 3 -->
        <div class="row">
          <div class="col-md-4">
            <div class="paper-thumbnail">
              <a href="https://nlmap-saycan.github.io/">
                <img
                  loading="lazy"
                  src="images/research/nlmap.jpg"
                  alt="paper thumbnail"
                  width="504"
                  height="300" />
              </a>
            </div>
          </div>
          <div class="col-md-8">
            <div class="paper-info" id="nlmap">
              <a href="https://nlmap-saycan.github.io/">
                <heading
                  >Open-vocabulary Queryable Scene Representations for Real
                  World Planning
                </heading>
              </a>
              <br />
              Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana
              Gopalakrishnan, Michael S. Ryoo, Austin Stone, Daniel Kappler
              <br />
              <strong>ICRA 2023</strong> (International Conference on Robotics
              and Automation)
              <br />
              <br />
              <a href="https://nlmap-saycan.github.io/">website</a>
              | <a href="https://arxiv.org/abs/2209.09874">paper</a> |
              <a href="javascript:toggleblock('nlmap_abs')">abstract</a> |
              <a
                shape="rect"
                href="javascript:togglebib('nlmap')"
                class="togglebib"
                >bibtex</a
              >
              |
              <a href="https://youtu.be/Q9CvvArq4ZA">talk video</a>
              <pre xml:space="preserve" style="display: none">
@inproceedings{chen2022nlmapsaycan,
    title={Open-vocabulary Queryable Scene Representations for Real World Planning},
    author={Boyuan Chen and Fei Xia and Brian Ichter and Kanishka Rao and Keerthana Gopalakrishnan and Michael S. Ryoo and Austin Stone and Daniel Kappler}
    booktitle={arXiv preprint arXiv:2209.09874},
    year={2022}
}
              </pre>
            </div>
            <p style="text-align: justify">
              <i id="nlmap_abs"
                >Large language models (LLMs) have unlocked new capabilities of
                task planning from human instructions. However, prior attempts
                to apply LLMs to real-world robotic tasks are limited by the
                lack of grounding in the surrounding scene. In this paper, we
                develop NLMap, an open-vocabulary and queryable scene
                representation to address this problem. NLMap serves as a
                framework to gather and integrate contextual information into
                LLM planners, allowing them to see and query available objects
                in the scene before generating a context-conditioned plan. NLMap
                first establishes a natural language queryable scene
                representation with Visual Language models (VLMs). An LLM based
                object proposal module parses instructions and proposes involved
                objects to query the scene representation for object
                availability and location. An LLM planner then plans with such
                information about the scene. NLMap allows robots to operate
                without a fixed list of objects nor executable options, enabling
                real robot operation unachievable by previous methods.</i
              >
            </p>
          </div>
        </div>

        <!--Paper 2 -->
        <div class="row">
          <div class="col-md-4">
            <div class="paper-thumbnail">
              <a href="https://buoyancy99.github.io/unsup-3d-keypoints/">
                <img
                  loading="lazy"
                  src="images/research/keypoint3D.jpg"
                  alt="paper thumbnail"
                  width="504"
                  height="300" />
              </a>
            </div>
          </div>
          <div class="col-md-8">
            <div class="paper-info" id="keypoint3D">
              <a href="https://buoyancy99.github.io/unsup-3d-keypoints/">
                <heading
                  >Unsupervised Learning of Visual 3D Keypoints for
                  Control</heading
                >
              </a>
              <br />
              Boyuan Chen, Pieter Abbeel, Deepak Pathak
              <br />
              <strong>ICML 2021</strong> (International Conference on Machine
              Learning)
              <br />
              <br />
              <a href="https://buoyancy99.github.io/unsup-3d-keypoints/"
                >website</a
              >
              | <a href="https://arxiv.org/abs/2106.07643">paper</a> |
              <a href="javascript:toggleblock('keypoint3D_abs')">abstract</a> |
              <a
                shape="rect"
                href="javascript:togglebib('keypoint3D')"
                class="togglebib"
                >bibtex</a
              >
              |
              <a href="https://github.com/buoyancy99/unsup-3d-keypoints"
                >code</a
              >
              |
              <a href="https://youtu.be/XnRzzxnlMOM">talk video</a>

              <pre xml:space="preserve" style="display: none">
@article{chen2021keypoint3D,
    author = {Chen, Boyuan and Abbeel, Pieter and Pathak, Deepak},
    title  = {Unsupervised Learning of Visual 3D Keypoints for Control},
    journal= {ICML},
    year   = {2021}
}
              </pre>
            </div>
            <p style="text-align: justify">
              <i id="keypoint3D_abs"
                >Learning sensorimotor control policies from high-dimensional
                images crucially relies on the quality of the underlying visual
                representations. Prior works show that structured latent space
                such as visual keypoints often outperforms unstructured
                representations for robotic control. However, most of these
                representations, whether structured or unstructured are learned
                in a 2D space even though the control tasks are usually
                performed in a 3D environment. In this work, we propose a
                framework to learn such a 3D geometric structure directly from
                images in an end-to-end unsupervised manner. The input images
                are embedded into latent 3D keypoints via a differentiable
                encoder which is trained to optimize both a multi-view
                consistency loss and downstream task objective. These discovered
                3D keypoints tend to meaningfully capture robot joints as well
                as object movements in a consistent manner across both time and
                3D space. The proposed approach outperforms prior state-of-art
                methods across a variety of reinforcement learning
                benchmarks.</i
              >
            </p>
          </div>
        </div>

        <!--Paper 1 -->
        <div class="row">
          <div class="col-md-4">
            <div class="paper-thumbnail">
              <a href="https://sites.google.com/view/sapnew/home">
                <img
                  loading="lazy"
                  src="images/research/sap.jpg"
                  alt="paper thumbnail"
                  width="504"
                  height="300" />
              </a>
            </div>
          </div>
          <div class="col-md-8">
            <div class="paper-info" id="sap">
              <a href="https://sites.google.com/view/sapnew/home">
                <heading
                  >Zero-shot Policy Learning with Spatial Temporal Reward
                  Decomposition on Contingency-aware Observation</heading
                >
              </a>
              <br />
              Boyuan Chen*, Huazhe Xu*, Yang Gao and Trevor Darrell
              <br />
              <strong>ICRA 2021</strong> (International Conference on Robotics
              and Automation)
              <br />
              <br />
              <a href="https://sites.google.com/view/sapnew/home">website</a> |
              <a href="https://arxiv.org/abs/1910.08143">paper</a> |
              <a href="javascript:toggleblock('sap_abs')">abstract</a> |
              <a
                shape="rect"
                href="javascript:togglebib('sap')"
                class="togglebib"
                >bibtex</a
              >
              | <a href="https://github.com/buoyancy99/sap">code</a> |

              <pre xml:space="preserve" style="display: none">
@article{DBLP:journals/corr/abs-1910-08143,
    author    = {Huazhe Xu and
                    Boyuan Chen and
                    Yang Gao and
                    Trevor Darrell},
    title     = {Scoring-Aggregating-Planning: Learning task-agnostic priors from interactions
                    and sparse rewards for zero-shot generalization},
    journal   = {CoRR},
    volume    = {abs/1910.08143},
    year      = {2019},
    url       = {http://arxiv.org/abs/1910.08143},
    eprinttype = {arXiv},
    eprint    = {1910.08143},
    timestamp = {Fri, 27 Nov 2020 15:04:16 +0100},
    biburl    = {https://dblp.org/rec/journals/corr/abs-1910-08143.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
    }
                            </pre
              >
            </div>
            <p style="text-align: justify">
              <i id="sap_abs"
                >It is a long-standing challenge to enable an intelligent agent
                to learn in one environment and generalize to an unseen
                environment without further data collection and finetuning. In
                this paper, we consider a zero shot generalization problem setup
                that complies with biological intelligent agents' learning and
                generalization processes. The agent is first presented with
                previous experiences in the training environment, along with
                task description in the form of trajectory-level sparse rewards.
                Later when it is placed in the new testing environment, it is
                asked to perform the task without any interaction with the
                testing environment. We find this setting natural for biological
                creatures and at the same time, challenging for previous
                methods. Behavior cloning, state-of-art RL along with other
                zero-shot learning methods perform poorly on this benchmark.
                Given a set of experiences in the training environment, our
                method learns a neural function that decomposes the sparse
                reward into particular regions in a contingency-aware
                observation as a per step reward. Based on such decomposed
                rewards, we further learn a dynamics model and use Model
                Predictive Control (MPC) to obtain a policy. Since the rewards
                are decomposed to finer-granularity observations, they are
                naturally generalizable to new environments that are composed of
                similar basic elements. We demonstrate our method on a wide
                range of environments, including a classic video game -- Super
                Mario Bros, as well as a robotic continuous control task. Please
                refer to the project page for more visualized results.
              </i>
            </p>
          </div>
        </div>

        <!--Paper 0 -->
        <div class="row">
          <div class="col-md-4">
            <div class="paper-thumbnail">
              <a href="https://sites.google.com/view/staghuntrpg">
                <img
                  loading="lazy"
                  src="images/research/rpg.jpg"
                  alt="paper thumbnail"
                  width="504"
                  height="300" />
              </a>
            </div>
          </div>
          <div class="col-md-8">
            <div class="paper-info" id="rpg">
              <a href="https://sites.google.com/view/staghuntrpg">
                <heading
                  >Discovering Diverse Multi-agent Strategic Behavior via Reward
                  Randomization</heading
                >
              </a>
              <br />
              Zhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang,
              Fei Fang, Simon Shaolei Du, Yu Wang, Yi Wu
              <br />
              <strong>ICLR 2021</strong> (International Conference on Learning
              Representations)
              <br />
              <br />
              <a href="https://sites.google.com/view/staghuntrpg">website</a> |
              <a href="https://arxiv.org/abs/2103.04564">paper</a> |
              <a href="javascript:toggleblock('rpg_abs')">abstract</a> |
              <a
                shape="rect"
                href="javascript:togglebib('rpg')"
                class="togglebib"
                >bibtex</a
              >
              | <a href="https://github.com/staghuntrpg/RPG">code</a> |

              <pre xml:space="preserve" style="display: none">
@misc{tang2021discovering,
    title={Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization}, 
    author={Zhenggang Tang and Chao Yu and Boyuan Chen and Huazhe Xu and Xiaolong Wang and Fei Fang and Simon Du and Yu Wang and Yi Wu},
    year={2021},
    eprint={2103.04564},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
                            </pre
              >
            </div>
            <p style="text-align: justify">
              <i id="rpg_abs"
                >We propose a simple, general and effective technique, Reward
                Randomization for discovering diverse strategic policies in
                complex multi-agent games. Combining reward randomization and
                policy gradient, we derive a new algorithm, Reward-Randomized
                Policy Gradient (RPG). RPG is able to discover multiple
                distinctive human-interpretable strategies in challenging
                temporal trust dilemmas, including grid-world games(MonsterHunt
                and Escalation) and a real-world web game Agar.io, where
                multiple equilibria exist but standard multi-agent policy
                gradient algorithms always converge to a fixed one with a
                sub-optimal payoff for every player even using state-of-the-art
                exploration techniques (including RND, DIAYN, MAVEN).
                Furthermore, with the set of diverse strategies from RPG, we can
                (1) achieve higher payoffs by fine-tuning the best policy from
                the set; and (2) obtain an adaptive agent by using this set of
                strategies as its training opponents.
              </i>
            </p>
          </div>
        </div>
      </div>
    </section>
    <!--Research Section End-->

    <!--Portfolio Section Start-->
    <section
      id="portfolio"
      class="portfolio pt-100 pb-70"
      data-scroll-index="3"
      style="display: none">
      <div class="container">
        <div class="row">
          <div class="col">
            <div class="heading text-center">
              <h2>MISC</h2>
            </div>
            <div class="portfolio-filter text-center">
              <ul>
                <!-- <li class="sel-item" data-filter="*">All</li> -->
                <li data-filter=".robot">Robots</li>
                <li data-filter=".food">Cooking</li>
                <li data-filter=".team">Teams</li>
              </ul>
            </div>
          </div>
        </div>
        <div class="row portfolio-items">
          <!--Portfolio Item-->
          <div class="col-lg-4 col-md-6 item robot">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/robot_kit.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Robomooc Robotics Kit</h6>
                <p>
                  I designed it with my friend, Kinsky. We sold it as an
                  education kit to schools. You can ride on it!
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item-->
          <div class="col-lg-4 col-md-6 item robot">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/robomaster.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Robomaster ICRA challenge</h6>
                <p>
                  DJI robomaster robot for ICRA AI Challenge. During my
                  undergrad, I was the captain of the team, leading the
                  development of autonomous algorithms in the robot shooting
                  challenge.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item-->
          <div class="col-lg-4 col-md-6 item robot">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/rover.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Autonomous Bogie Rover</h6>
                <p>
                  My personal robot that can handle a variety of terrains. I did
                  everything from machanical design, electronics to programming.
                  It uses computer vision to autonomously follow me and avoid
                  obstables.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item-->
          <div class="col-lg-4 col-md-6 item robot">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/frc17.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>FRC 2017 Robot</h6>
                <p>
                  In 2017, I founded my high school's first FRC team. We didn't
                  have the mentorship nor funding we need, but the team did
                  amazing. I did the majority of the design.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item-->
          <div class="col-lg-4 col-md-6 item robot">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/pr2.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>PR2 in RLL</h6>
                <p>
                  In 2021, I graduated from UC Berkeley, where I spent some
                  amazing time doing research in robotics learning lab.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item-->
          <div class="col-lg-4 col-md-6 item robot">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/drone.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Autonomous Drone</h6>
                <p>
                  An autonomous drone which I built and coded. I installed a
                  camera a mini railgun on it to track and aim at the target I
                  select.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item-->
          <div class="col-lg-4 col-md-6 item robot">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/ftc17.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>FTC 2017 Robot</h6>
                <p>
                  Our FTC competition robot in 2017, when I became the captain
                  of the team. It's my team's first robot designed with CAD. The
                  robot won the east China regional.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item-->
          <div class="col-lg-4 col-md-6 item robot">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/ftc16.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>My first ftc robot</h6>
                <p>
                  In 2016, I participanted in robotics competition for the first
                  time. This is a super cool robot which marks the beginning of
                  my robotics journey.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item-->
          <div class="col-lg-4 col-md-6 item robot">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/frc18.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>FRC 2018 Robot</h6>
                <p>
                  After my graduation from high school, I continued mentoring
                  the team. My successor Xinpei designed the robot under my
                  mentorship.
                </p>
              </div>
            </div>
          </div>

          <!--Portfolio Item-->
          <div class="col-lg-4 col-md-6 item team">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/rm_team.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Robomaster Team</h6>
                <p>
                  In 2019, I was the captain of Berkeley's team in ICRA
                  Robomaster AI Challenge. I co-founded the team and lead 20
                  student developing autonomous robots.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item-->
          <div class="col-lg-4 col-md-6 item team">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/chess.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>MIT Chess Club</h6>
                <p>
                  I became one of the execs at MIT Chess Club in 2022. It was a
                  great time to organize events and hangout with the team!
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item-->
          <div class="col-lg-4 col-md-6 item team">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/frc_team.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>FRC Team</h6>
                <p>
                  In 2017, I founded my high school's first FRC team. I worked
                  as both captain and mentor. We won the Rookie All Star Award
                  at CRC 2017.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item 1-->
          <div class="col-lg-4 col-md-6 item food">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/newyear22.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Chinese New Year 2022</h6>
                <p>
                  To celebrate Chinese New Year 2022, I made a big dinner with
                  my friend Maohao Shen at MIT. MITCSSA awarded me the title
                  Master Chef MIT for my Peking duck in their cooking
                  competition.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item 2-->
          <div class="col-lg-4 col-md-6 item food">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/chicken_noodle.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Home Style Noodle with Braised Chicken<br />黄焖鸡面</h6>
                <p>I cooked 黄焖鸡 during COVID-19 quarantine!</p>
              </div>
            </div>
          </div>
          <!--Portfolio Item 3-->
          <div class="col-lg-4 col-md-6 item food">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/dongporou.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Soy sauce braised pork</h6>
                <p>
                  I made Dongporou (东坡肉) during Thanksgiving 2023. The best
                  Soy sauce braised pork I've every made!
                </p>
              </div>
            </div>
          </div>

          <!--Portfolio Item 4-->
          <div class="col-lg-4 col-md-6 item food">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/chicken_soup.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Chicken Soup with Mushroom<br />(松茸鸡汤)</h6>
                <p>
                  Traditional Chinese chicken soup with dried matsutake
                  mushroom.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item 5-->
          <div class="col-lg-4 col-md-6 item food">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/2023-chinese-ny.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Chinese New Year 2023</h6>
                <p>
                  I cooked 5 dished for 2023 Chinese New Year. All of them are
                  amazing! The dishes are steam eel, egg plant with minced meat,
                  soy sauce braised pork with bamboo shoots, chinese chicken
                  soup with bamboo-mushroom and stir-fried Chinese chives.
                </p>
              </div>
            </div>
          </div>

          <!--Portfolio Item 6-->
          <div class="col-lg-4 col-md-6 item food">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/birthday21.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Birthday Noodle (长寿面)</h6>
                <p>
                  I made my roommate and long time friend Haoyuan a bowl of
                  traditional birthday noodle in 2021, when he turned 23.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item 7-->
          <div class="col-lg-4 col-md-6 item food">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/brisket.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Potato Braised Beef Brisket</h6>
                <p>I cooked beef brisket (土豆炖排骨) in COVID-19 lockdown.</p>
              </div>
            </div>
          </div>
          <!--Portfolio Item 8-->
          <div class="col-lg-4 col-md-6 item food">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/lamb.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>Lamb Croutons</h6>
                <p>
                  During the COVID-19 pandemic, I tried to make Lamb Croutons
                  following Gordon Ramsay's tutorial.
                </p>
              </div>
            </div>
          </div>
          <!--Portfolio Item 9-->
          <div class="col-lg-4 col-md-6 item food">
            <div class="item-content">
              <img
                loading="lazy"
                src="images/portfolio/tofu-stew.jpg"
                alt=""
                width="600"
                height="500" />
              <div class="item-overlay">
                <h6>XO sauce Tofu Stew with mushrooms</h6>
                <p>
                  Tofu stew cooked with various mushrooms and XO sauce. The
                  Umami flavor will burst into your mouse - it's finished in 2
                  minutes by all my friends.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!--Portfolio Section End-->

    <!--Owl Carousel css-->
    <link rel="stylesheet" href="css/owl.carousel.min.css" />
    <link rel="stylesheet" href="css/owl.theme.default.min.css" />

    <!--Blog Section Start-->
    <section
      id="blogs"
      class="blogs pt-100 pb-100 bg-gray"
      data-scroll-index="4"
      style="display: none">
      <div class="container">
        <div class="row">
          <div class="col text-center">
            <div class="heading text-center">
              <h2>Blog</h2>
            </div>
          </div>
        </div>
        <div class="row">
          <div class="owl-carousel owl-theme">
            <!--Blogs Item-->
            <div class="blog-item">
              <div class="blog-img">
                <a href="blogs/boba_ranking.html">
                  <img
                    loading="lazy"
                    src="images/blog/001_boba/thumbnail.jpg"
                    alt=""
                    width="550"
                    height="350" />
                </a>
              </div>
              <div class="blog-content">
                <h3>Best computer science schools ranked by boba</h3>
                <p>
                  Many people underestimate the importance of boba when they
                  choose grad school. For those who don't know what boba is ...
                </p>
                <div class="blog-meta">
                  <span class="more">
                    <a href="blogs/boba_ranking.html">Read More</a>
                  </span>
                  <span class="date"> 20/Jun/2022 </span>
                </div>
              </div>
            </div>
            <div class="blog-item">
              <div class="blog-img">
                <a href="blogs/jushenzhineng.html">
                  <img
                    loading="lazy"
                    src="images/blog/002_jushenzhineng/thumbnail.jpg"
                    alt=""
                    width="550"
                    height="350" />
                </a>
              </div>
              <div class="blog-content">
                <h3>随笔：我们应该如何看待具身智能</h3>
                <p>
                  以ChatGPT为代表的大模型让我们瞥见了未来的一隅。机器人大模型在过去一年里出现在了几乎每一个机器人公司的PPT里。那么大语言模型的思路会给我们带通用机器人么？
                  ...
                </p>
                <div class="blog-meta">
                  <span class="more">
                    <a href="blogs/jushenzhineng.html">Read More</a>
                  </span>
                  <span class="date"> 16/Jun/2024 </span>
                </div>
              </div>
            </div>
            <div class="blog-item">
              <div class="blog-img">
                <a href="blogs/embodied_ai.html">
                  <img
                    loading="lazy"
                    src="images/blog/003_embodied_ai/thumbnail.jpg"
                    alt=""
                    width="550"
                    height="350" />
                </a>
              </div>
              <div class="blog-content">
                <h3>
                  The epic quest to Embodied AI and general-purpose robots
                </h3>
                <p>
                  ChatGPT has given us a glimpse of the future. So, will the
                  same bring us general-purpose robots? ...
                </p>
                <div class="blog-meta">
                  <span class="more">
                    <a href="blogs/embodied_ai.html">Read More</a>
                  </span>
                  <span class="date"> 16/Jun/2024 </span>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="row">
          <div class="col text-center">
            <div class="blog-button pt-40">
              <a class="main-btn" href="blogs-page.html">View More</a>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!--Blog Section End-->

    <!--Footer Start-->
    <footer id="footer" class="pt-50 pb-50" style="display: none">
      <div class="container">
        <!-- <div class="row text-center">
                    <div class="col-md-3 col-sm-6">
                        <div class="contact-info">
                            <h5>Boyuan</h5>
                            <p>lorem Ipsum donor sit.</p>
                        </div>
                    </div>
                    <div class="col-md-3 col-sm-6">
                        <div class="contact-info">
                            <h5>Phone No.</h5>
                            <p>(+1) 123 456 7890</p>
                        </div>
                    </div>
                    <div class="col-md-3 col-sm-6">
                        <div class="contact-info">
                            <h5>Email</h5>
                            <p>info@example.com</p>
                        </div>
                    </div>
                    <div class="col-md-3 col-sm-6">
                        <div class="contact-info">
                            <h5>Address</h5>
                            <p>123 lorem ipsum New York, USA.</p>
                        </div>
                    </div>
                </div> -->
        <div class="row text-center">
          <div class="col-md-12">
            <p class="copy pt-30">
              Boyuan Chen &copy; 2022. All Right Reserved.
            </p>
          </div>
        </div>
      </div>
    </footer>
    <!--Footer End-->

    <!--Jquery js-->
    <script src="js/jquery.min.js"></script>
    <!--Bootstrap js-->
    <script src="js/bootstrap.min.js"></script>
    <!--Stellar js-->
    <script src="js/jquery.stellar.js"></script>
    <!--Animated Headline js-->
    <script src="js/animated.headline.js"></script>
    <!--Owl Carousel js-->
    <script src="js/owl.carousel.min.js"></script>
    <!--ScrollIt js-->
    <script src="js/scrollIt.min.js"></script>
    <!--Isotope js-->
    <script src="js/isotope.pkgd.min.js"></script>
    <!--Magnific Popup js-->
    <script src="js/jquery.magnific-popup.min.js"></script>
    <!--Site Main js-->
    <script src="js/main.js"></script>
    <!--Hide unless clicked js-->
    <script src="js/hidebib.js"></script>

    <!--Hide paper abstract by default-->
    <script xml:space="preserve">
      hideblock("df_abs");
      hideblock("dittogym_abs");
      hideblock("spatialvlm_abs");
      hideblock("ramp_abs");
      hideblock("eil_abs");
      hideblock("nlmap_abs");
      hideblock("keypoint3D_abs");
      hideblock("rpg_abs");
      hideblock("sap_abs");
    </script>
  </body>
</html>
